# src/retrain.py
"""
Retraining pipeline.

- Scans all CSV files inside data/new/
- Loads all labeled examples
- Prints a summary (original behavior)
- If there is data â†’ trains a simple fine-tuned model
- Saves model + metrics to models/fine_tuned/
- If no data â†’ skips retraining
- Uses Hugging Face Transformers Trainer API
"""

from pathlib import Path
from typing import List
import json

import pandas as pd
from datasets import Dataset
from sklearn.metrics import accuracy_score, f1_score
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

from .data import list_new_data_files, load_labeled_data


BASE_MODEL_NAME = "cardiffnlp/twitter-roberta-base-sentiment-latest"
MODEL_OUT_DIR = Path("models/fine_tuned")
METRICS_PATH = Path("models/fine_tuned/metrics.json")


def print_retraining_plan(files: List[Path], dfs: List[pd.DataFrame]) -> None:
    """Print original summary of labeled examples per file."""
    print("")
    print("ğŸ“„ Summary of new labeled data:")
    total_examples = 0

    for path, df in zip(files, dfs):
        n = len(df)
        total_examples += n
        print(f"   â€¢ {path} â†’ {n} examples")

    if total_examples == 0:
        print("\nâš ï¸ New files found, but they contain NO valid labeled examples.")
    else:
        print(f"\nTotal new labeled examples: {total_examples}\n")

    print("Planned retraining steps:")
    print("  1. Merge new CSV files")
    print("  2. Train/validation split (80/20)")
    print("  3. Fine-tune pretrained model")
    print("  4. Evaluate")
    print("  5. Save fine-tuned model")
    print("")


#helpers
def _load_all_dfs(files: List[Path]) -> List[pd.DataFrame]:
    """Load each CSV into a DataFrame using load_labeled_data."""
    dfs = []
    for path in files:
        texts, labels = load_labeled_data(path)
        df = pd.DataFrame({"text": texts, "label": labels}).dropna()
        if not df.empty:
            df["label"] = df["label"].astype(int)
        dfs.append(df)
    return dfs


def _compute_metrics(pred):
    preds = pred.predictions.argmax(-1)
    labels = pred.label_ids
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1": f1_score(labels, preds, average="macro"),
    }


# Main retraining function
def main() -> None:
    files = list_new_data_files()

    if not files:
        print("âœ… No new labeled data found in data/new/. Skipping retraining.")
        return

    print(f"ğŸ“¥ Detected {len(files)} new CSV file(s).")
    for f in files:
        print(f"   - {f}")

    # Load CSVs
    dfs = _load_all_dfs(files)

    # Print original-style summary
    print_retraining_plan(files, dfs)

    # Merge all non-empty DataFrames
    merged = pd.concat([df for df in dfs if not df.empty], ignore_index=True)

    if merged.empty:
        print("âŒ No usable labeled data. Retraining aborted.")
        return

    # Simple 80/20 split
    split_idx = int(len(merged) * 0.8)
    train_df = merged.iloc[:split_idx].reset_index(drop=True)
    val_df = merged.iloc[split_idx:].reset_index(drop=True)

    print(f"ğŸ”€ Train size: {len(train_df)}, Val size: {len(val_df)}")

    # Convert to HF Datasets
    train_ds = Dataset.from_pandas(train_df, preserve_index=False)
    val_ds = Dataset.from_pandas(val_df, preserve_index=False)

    print("ğŸ”¤ Loading tokenizer and base model...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)
    model = AutoModelForSequenceClassification.from_pretrained(
        BASE_MODEL_NAME, num_labels=3
    )

    def tokenize(batch):
        return tokenizer(
            batch["text"],
            truncation=True,
            padding="max_length",
            max_length=128,
        )

    print("ğŸ” Tokenizing...")
    train_ds = train_ds.map(tokenize, batched=True)
    val_ds = val_ds.map(tokenize, batched=True)

    remove_cols_train = [
        c for c in train_ds.column_names if c not in ["input_ids", "attention_mask", "label"]
    ]
    remove_cols_val = [
        c for c in val_ds.column_names if c not in ["input_ids", "attention_mask", "label"]
    ]
    train_ds = train_ds.remove_columns(remove_cols_train).with_format("torch")
    val_ds = val_ds.remove_columns(remove_cols_val).with_format("torch")

    MODEL_OUT_DIR.mkdir(parents=True, exist_ok=True)

    print("âš™ï¸ Fine-tuning for 1 epoch...")
    # NOTE: only basic arguments, compatible with older transformers versions
    args = TrainingArguments(
        output_dir=str(MODEL_OUT_DIR),
        num_train_epochs=1,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=16,
        learning_rate=2e-5,
        logging_steps=10,
    )

    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=train_ds,
        eval_dataset=val_ds,
        compute_metrics=_compute_metrics,
    )

    trainer.train()

    print("ğŸ“Š Evaluating...")
    metrics = trainer.evaluate()
    print("Metrics:", metrics)

    print("ğŸ’¾ Saving fine-tuned model and metrics...")
    trainer.save_model(str(MODEL_OUT_DIR))

    METRICS_PATH.parent.mkdir(parents=True, exist_ok=True)
    with METRICS_PATH.open("w") as f:
        json.dump(metrics, f, indent=2)

    print(f"âœ… Model saved â†’ {MODEL_OUT_DIR}")
    print(f"ğŸ“˜ Metrics saved â†’ {METRICS_PATH}")


if __name__ == "__main__":
    main()
